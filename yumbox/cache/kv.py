import functools
import hashlib
import json
import logging
import os
import uuid

import lmdb
import msgpack
import numpy as np
from lmdb import Environment

from yumbox.config import BFG

logger = logging.getLogger(__name__)


class LMDBMultiIndex:
    def __init__(self, db_name: str, folder: str, map_size: int = 10485760):
        """
        Initialize LMDB database.
        :param db_name: Name of the database (e.g., "mydata").
        :param folder: Folder where DB files will be stored.
        :param map_size: Max size in bytes (default 10MB, adjust as needed).
        """
        self.folder = folder
        os.makedirs(folder, exist_ok=True)
        self.db_path = os.path.join(folder, db_name)

        self.env: Environment = lmdb.open(self.db_path, map_size=map_size, lock=False)
        self.data_prefix = b"\t$DATA\t"  # Namespace for actual data

    def _data_id_to_bytes(self, data_id: str) -> bytes:
        """Convert data ID to bytes."""
        return self.data_prefix + data_id.encode()

    def exists(self, key: str) -> bool:
        """Check if a key exists."""
        with self.env.begin() as txn:
            key_bytes = key.encode()
            return txn.get(key_bytes) is not None

    def batch_exists(self, keys: list[str]) -> list[bool]:
        """Check existence of multiple keys in batch."""
        result = []
        with self.env.begin() as txn:
            for key in keys:
                key_bytes = key.encode()
                result.append(txn.get(key_bytes) is not None)
        return result

    def create(self, key: str, data: dict, data_id: str | None = None) -> str:
        """
        Create a new entry. If data_id is provided, use it; otherwise, generate one.
        :return: The data_id used.
        """
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            if txn.get(key_bytes):
                raise ValueError(f"Key {key} already exists")

            # If no data_id provided, generate a new UUID
            if data_id is None:
                data_id = str(uuid.uuid4())
            data_id_bytes = self._data_id_to_bytes(data_id)

            # Store the data if it doesn't exist
            if not txn.get(data_id_bytes):
                txn.put(data_id_bytes, msgpack.packb(data))

            # Map key to data_id
            txn.put(key_bytes, data_id.encode())
            return data_id

    def update(self, key: str, data: dict):
        """Update the data for an existing key (affects all keys referencing the same data)."""
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            data_id = txn.get(key_bytes)
            if not data_id:
                raise KeyError(f"Key {key} does not exist")

            data_id_bytes = self._data_id_to_bytes(data_id.decode())
            txn.put(data_id_bytes, msgpack.packb(data))

    def delete(self, key: str):
        """Delete a key. Data remains if referenced by other keys."""
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            if not txn.get(key_bytes):
                raise KeyError(f"Key {key} does not exist")
            txn.delete(key_bytes)

    def get(self, key: str) -> dict | None:
        """Get data by key."""
        with self.env.begin() as txn:
            key_bytes = key.encode()
            data_id = txn.get(key_bytes)
            if data_id is None:
                raise KeyError(f"Key {key} does not exist")
            else:
                data_id_bytes = self._data_id_to_bytes(data_id.decode())
                data_bytes = txn.get(data_id_bytes)
                if data_bytes is None:
                    raise KeyError(
                        f"Inner key {data_id.decode()} for key {key} does not exist"
                    )
                else:
                    return msgpack.unpackb(data_bytes, raw=False)
        return None

    def get_dataset(self, mode: str = "unique") -> dict:
        """
        Export database to a dictionary, with options to handle duplicate values.

        :param mode: How to handle data with multiple keys:
            - "unique": Return unique data values with one representative key (default)
            - "grouped": Group keys by data_id (returns dict of data with lists of keys)
            - "all": Return all keys with their data (including duplicates)
        :return: Dictionary of data in the requested format
        """
        if mode == "grouped":
            return self._get_grouped_by_data()
        elif mode == "all":
            return self._get_all_data()
        else:  # Default to unique
            return self._get_unique_data()

    def _get_grouped_by_data(self):
        """Group keys by data_id (keys sharing same data)."""
        grouped_data = {}  # data_id -> (data, [keys])
        with self.env.begin() as txn:
            cursor = txn.cursor()
            # First pass: collect all key -> data_id mappings
            key_to_data_id = {}
            for key_bytes, value_bytes in cursor:
                if key_bytes.startswith(self.data_prefix):
                    continue  # Skip data entries
                key = key_bytes.decode()
                data_id = value_bytes.decode()
                key_to_data_id[key] = data_id

                # Initialize the group for this data_id if it doesn't exist
                if data_id not in grouped_data:
                    data_id_bytes = self._data_id_to_bytes(data_id)
                    data_bytes = txn.get(data_id_bytes)
                    if data_bytes:
                        data = msgpack.unpackb(data_bytes, raw=False)
                        grouped_data[data_id] = (data, [])

            # Second pass: group keys by data_id
            for key, data_id in key_to_data_id.items():
                if data_id in grouped_data:
                    grouped_data[data_id][1].append(key)

        # Transform to more usable format
        result = {}
        for data_id, (data, keys) in grouped_data.items():
            result[data_id] = {"data": data, "keys": keys}
        return result

    def _get_unique_data(self):
        """Return unique data values with one representative key."""
        records = {}
        seen_data_ids = set()
        with self.env.begin() as txn:
            cursor = txn.cursor()
            for key_bytes, value_bytes in cursor:
                if key_bytes.startswith(self.data_prefix):
                    continue  # Skip data entries
                key = key_bytes.decode()
                data_id = value_bytes.decode()

                # Only process each data_id once
                if data_id not in seen_data_ids:
                    seen_data_ids.add(data_id)
                    data_id_bytes = self._data_id_to_bytes(data_id)
                    data_bytes = txn.get(data_id_bytes)
                    if data_bytes:
                        data = msgpack.unpackb(data_bytes, raw=False)
                        records[key] = data
        return records

    def _get_all_data(self):
        """Return all keys with their data (including duplicates)."""
        records = {}
        with self.env.begin() as txn:
            cursor = txn.cursor()
            for key_bytes, value_bytes in cursor:
                if key_bytes.startswith(self.data_prefix):
                    continue  # Skip data entries
                key = key_bytes.decode()
                data_id = value_bytes.decode()
                data_id_bytes = self._data_id_to_bytes(data_id)
                data_bytes = txn.get(data_id_bytes)
                if data_bytes:
                    data = msgpack.unpackb(data_bytes, raw=False)
                    records[key] = data
        return records

    def close(self):
        """Close the database."""
        self.env.close()


class LMDB:
    def __init__(self, db_name: str, folder: str, map_size: int = 2**30):
        """
        Initialize LMDB database.
        :param db_name: Name of the database (e.g., "mydata").
        :param folder: Folder where DB files will be stored.
        :param map_size: Max size in bytes (default 10MB, adjust as needed).
        """
        self.folder = folder
        os.makedirs(folder, exist_ok=True)
        self.db_path = os.path.join(folder, db_name)

        self.env: Environment = lmdb.open(self.db_path, map_size=map_size, lock=False)

    def exists(self, key: str) -> bool:
        """Check if a key exists."""
        with self.env.begin() as txn:
            key_bytes = key.encode()
            return txn.get(key_bytes) is not None

    def batch_exists(self, keys: list[str]) -> list[bool]:
        """Check existence of multiple keys in batch."""
        result = []
        with self.env.begin() as txn:
            for key in keys:
                key_bytes = key.encode()
                result.append(txn.get(key_bytes) is not None)
        return result

    def upsert(self, key: str, data: dict) -> bool:
        """
        Upsert an entry.
        :return: True if it was written.
        """
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            return txn.put(key_bytes, msgpack.packb(data))

    def create(self, key: str, data: dict) -> bool:
        """
        Create a new entry.
        :return: True if it was written or raises ValueError if already exists.
        """
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            if txn.get(key_bytes):
                raise ValueError(f"Key {key} already exists")

            return txn.put(key_bytes, msgpack.packb(data))

    def update(self, key: str, data: dict):
        """Update the data for an existing key."""
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            if not txn.get(key_bytes):
                raise KeyError(f"Key {key} does not exist")

            txn.put(key_bytes, msgpack.packb(data))

    def delete(self, key: str):
        """Delete a key and its associated data."""
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            if not txn.get(key_bytes):
                raise KeyError(f"Key {key} does not exist")
            txn.delete(key_bytes)

    def get(self, key: str) -> dict | None:
        """Get data by key."""
        with self.env.begin() as txn:
            key_bytes = key.encode()
            data_bytes = txn.get(key_bytes)
            if data_bytes is None:
                raise KeyError(f"Key {key} does not exist")
            else:
                return msgpack.unpackb(data_bytes, raw=False)
        return None

    def get_dataset(self) -> dict[str, dict]:
        """Export entire database to a dictionary."""
        records = {}
        with self.env.begin() as txn:
            cursor = txn.cursor()
            for key_bytes, data_bytes in cursor:
                key = key_bytes.decode()
                data = msgpack.unpackb(data_bytes, raw=False)
                records[key] = data
        return records

    def close(self):
        """Close the database."""
        self.env.close()


class VectorLMDB:
    def __init__(self, db_name: str, folder: str, map_size: int = 30 * 2**30):
        """
        Initialize LMDB database for numpy arrays.
        :param db_name: Name of the database (e.g., "mydata").
        :param folder: Folder where DB files will be stored.
        :param map_size: Max size in bytes (default 10GB, adjust as needed).
        """
        self.folder = folder
        os.makedirs(folder, exist_ok=True)
        self.db_path = os.path.join(folder, db_name)
        self.env: Environment = lmdb.open(self.db_path, map_size=map_size, lock=False)

    def _serialize_array(self, arr: np.ndarray) -> bytes:
        """Serialize numpy array to compressed bytes."""
        import gzip
        import io

        buffer = io.BytesIO()
        with gzip.GzipFile(fileobj=buffer, mode="wb") as f:
            np.save(f, arr, allow_pickle=False)
        return buffer.getvalue()

    def _deserialize_array(self, data_bytes: bytes) -> np.ndarray:
        """Deserialize bytes back to numpy array."""
        import gzip
        import io

        buffer = io.BytesIO(data_bytes)
        with gzip.GzipFile(fileobj=buffer, mode="rb") as f:
            return np.load(f, allow_pickle=False)

    def exists(self, key: str) -> bool:
        """Check if a key exists."""
        with self.env.begin() as txn:
            key_bytes = key.encode()
            return txn.get(key_bytes) is not None

    def batch_exists(self, keys: list[str]) -> list[bool]:
        """Check existence of multiple keys in batch."""
        result = []
        with self.env.begin() as txn:
            for key in keys:
                key_bytes = key.encode()
                result.append(txn.get(key_bytes) is not None)
        return result

    def upsert(self, key: str, array: np.ndarray) -> bool:
        """
        Upsert a numpy array.
        :return: True if it was written.
        """
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            serialized_data = self._serialize_array(array)
            return txn.put(key_bytes, serialized_data)

    def create(self, key: str, array: np.ndarray) -> bool:
        """
        Create a new entry.
        :return: True if it was written or raises ValueError if already exists.
        """
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            if txn.get(key_bytes):
                raise ValueError(f"Key {key} already exists")
            serialized_data = self._serialize_array(array)
            return txn.put(key_bytes, serialized_data)

    def update(self, key: str, array: np.ndarray):
        """Update the array for an existing key."""
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            if not txn.get(key_bytes):
                raise KeyError(f"Key {key} does not exist")
            serialized_data = self._serialize_array(array)
            txn.put(key_bytes, serialized_data)

    def delete(self, key: str):
        """Delete a key and its associated array."""
        with self.env.begin(write=True) as txn:
            key_bytes = key.encode()
            if not txn.get(key_bytes):
                raise KeyError(f"Key {key} does not exist")
            txn.delete(key_bytes)

    def get(self, key: str) -> np.ndarray:
        """Get array by key."""
        with self.env.begin() as txn:
            key_bytes = key.encode()
            data_bytes = txn.get(key_bytes)
            if data_bytes is None:
                raise KeyError(f"Key {key} does not exist")
            return self._deserialize_array(data_bytes)

    def get_info(self, key: str) -> dict[str, str | tuple]:
        """
        Get array metadata without loading the full array.
        :param key: String key to inspect
        :return: Dictionary with dtype, shape, and size info
        :raises KeyError: if key does not exist
        """
        array = self.get(key)  # For now, we need to load it
        return {
            "dtype": str(array.dtype),
            "shape": array.shape,
            "size": array.size,
            "nbytes": array.nbytes,
        }

    def list_keys(self) -> list[str]:
        """Get all keys in the database."""
        keys = []
        with self.env.begin() as txn:
            cursor = txn.cursor()
            for key_bytes, _ in cursor:
                keys.append(key_bytes.decode())
        return keys

    def get_dataset(self) -> dict[str, np.ndarray]:
        """Export entire database to a dictionary."""
        records = {}
        with self.env.begin() as txn:
            cursor = txn.cursor()
            for key_bytes, data_bytes in cursor:
                key = key_bytes.decode()
                array = self._deserialize_array(data_bytes)
                records[key] = array
        return records

    def close(self):
        """Close the database."""
        self.env.close()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def bulk_upsert(self, data: dict[str, np.ndarray]) -> int:
        """
        Bulk upsert multiple numpy arrays.
        :param data: Dictionary of {key: array} pairs to insert
        :return: Number of items successfully inserted
        """
        # Pre-serialize all data outside the transaction
        serialized_items = []
        for key, array in data.items():
            key_bytes = key.encode()
            serialized_data = self._serialize_array(array)
            serialized_items.append((key_bytes, serialized_data))

        # Bulk insert using putmulti
        with self.env.begin(write=True) as txn:
            cursor = txn.cursor()
            consumed, added = cursor.putmulti(serialized_items)
            return added

    def bulk_upsert_from_lists(self, keys: list[str], arrays: list[np.ndarray]) -> int:
        """
        Bulk upsert from separate key and array lists.
        :param keys: List of string keys
        :param arrays: List of numpy arrays (must match keys length)
        :return: Number of items successfully inserted
        """
        if len(keys) != len(arrays):
            raise ValueError("Keys and arrays lists must have same length")

        # Pre-serialize all data
        serialized_items = []
        for key, array in zip(keys, arrays):
            key_bytes = key.encode()
            serialized_data = self._serialize_array(array)
            serialized_items.append((key_bytes, serialized_data))

        # Bulk insert
        with self.env.begin(write=True) as txn:
            cursor = txn.cursor()
            consumed, added = cursor.putmulti(serialized_items)
            return added

    def bulk_get(self, keys: list[str]) -> dict[str, np.ndarray]:
        """# NO PERFORMANCE DIFFERENCE
        Bulk get multiple arrays by keys.
        :param keys: List of string keys to retrieve
        :return: Dictionary of {key: array} for found keys (missing keys are omitted)
        """
        result = {}
        with self.env.begin() as txn:
            for key in keys:
                key_bytes = key.encode()
                data_bytes = txn.get(key_bytes)
                if data_bytes is not None:
                    result[key] = self._deserialize_array(data_bytes)
        return result


def lmdb_cache(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        cache_dir = BFG["cache_dir"]

        func_name = func.__name__
        cache_db_path = os.path.join(cache_dir, func_name) if cache_dir else None

        cache = {}
        keys = kwargs.get("keys", [])
        keys = set(keys)

        if cache_db_path and os.path.exists(cache_db_path):
            logger.info(f"Loading cache for {func_name} from {cache_db_path}")
            with VectorLMDB(func_name, cache_dir) as db:
                # If keys are specified, load only those
                if keys:
                    for key in keys:
                        if db.exists(key):
                            cache[key] = db.get(key)
                else:
                    # Load all keys
                    cache = db.get_dataset()
            logger.info(f"Loaded cache for {func_name} from {cache_db_path}")

        cache = dict(keys=cache.keys(), values=cache.values())
        cache = func(*args, **kwargs, cache=cache, cache_file=cache_db_path)

        if cache_db_path:
            logger.info(f"Saving cache for {func_name} to {cache_db_path}")
            with VectorLMDB(func_name, cache_dir) as db:
                db.bulk_upsert(cache)
            logger.info(f"Saved cache for {func_name} to {cache_db_path}")

        return cache

    return wrapper


def lmdb_cache_kwargs_list_hash(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        cache_dir = BFG["cache_dir"]

        func_name = func.__name__

        if "cache_kwargs" not in kwargs or not kwargs["cache_kwargs"]:
            logger.warning(
                f"Skipped loading cache because kwargs was empty for {func_name}"
            )
            return func(*args, **kwargs, cache=None, cache_file=None)

        cache_dict = {}
        for c in kwargs["cache_kwargs"]:
            cache_dict[c] = kwargs[c]

        func_kwargs_hash = hashlib.md5(
            json.dumps(cache_dict, sort_keys=True, default=str).encode()
        ).hexdigest()

        db_name = f"{func_name}_{func_kwargs_hash}"
        cache_db_path = os.path.join(cache_dir, db_name) if cache_dir else None

        cache = {}
        keys = kwargs.get("keys", [])
        keys = set(keys)

        if cache_db_path and os.path.exists(cache_db_path):
            logger.info(f"Loading cache for {func_name} from {cache_db_path}")
            with VectorLMDB(db_name, cache_dir) as db:
                # If keys are specified, load only those
                if keys:
                    for key in keys:
                        if db.exists(key):
                            cache[key] = db.get(key)
                else:
                    # Load all keys
                    cache = db.get_dataset()
            logger.info(f"Loaded cache for {func_name} from {cache_db_path}")

        cache = dict(keys=cache.keys(), values=cache.values())
        cache = func(*args, **kwargs, cache=cache, cache_file=cache_db_path)

        if cache_db_path:
            logger.info(f"Saving cache for {func_name} to {cache_db_path}")
            with VectorLMDB(db_name, cache_dir) as db:
                db.bulk_upsert(cache)
            logger.info(f"Saved cache for {func_name} to {cache_db_path}")

        for key in keys:
            if key not in cache:
                raise ValueError(f"*Dataset ERROR: key {key} not in results.")

        return cache

    return wrapper
